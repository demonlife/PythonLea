* Basic Python
** 判断
   if x: pass
   只要x是非零数值、非空字符串、非空list, 非None， 非空dict等，就判断为True，否则为False
** set
   set的初始化:
   s = set() #空set
   s = set([1, 2, 3, 3]) #set必须使用list来初始化, 并且set会除去重复的元素
   set的原理和dict一样, 不能放入可变对象
   参看如下的实例:
   s = set([[1, 2], 3, 4]) # error
   s = set([(1, 2), 3, 4]) # right
   set可以做集合运算, list不可以
   s1 & s2 #交集
   s1 | s2 #并集
   ...
** 不变对象
   对于不变对象来说，调用对象自身的任意方法，也不会改变该对象自身的内容
** 函数
   完全可以把一个函数名赋值给一个变量,  相当于给这个函数起了一个"别名"
   a = abs
   a(-1) #=> 1
   python中对匿名函数提供了有限的支持, 就是只能有一个表达式, 不用写return, 返回值就是该表达式的结果
   python默认将一个函数中赋值的变量视为本地变量（局部变量）
*** functools
    偏函数
    import functools
    int2 = functools.partial(int, base=2)
    int2('10000')
    functools.partial的作用就是，把一个函数的某些参数（不管有没有默认值）给固定住（也就是设置默认值），
    返回一个新的函数，调用这个新函数会更简单
    创建偏函数时，要从右到左固定参数
*** 默认参数
    定义默认参数必须指向不变的对象, 否则会出现一些很奇怪的问题
    对于任何函数, 通过类似func(*args, **kw)的形式调用它，不管它的参数是如何定义的
*** 递归调用
    解决递归调用栈溢出的方法是通过尾递归优化。
    尾递归是指，在函数返回的时候，调用自身本身，并且，return语句不能包含表达式。
    这样，编译器或者解释器就可以把尾递归做优化，使递归本身无论调用多少次，都只占用一个栈帧，
    不会出现栈溢出的情况
    尾调用的定义: http://zh.wikipedia.org/zh-cn/%E5%B0%BE%E8%B0%83%E7%94%A8
    但是python没有对尾递归做优化, 一个针对尾递归优化的decorator,参考源码:
    http://code.activestate.com/recipes/474088-tail-call-optimization-decorator/

    针对尾递归优化的语言可以通过尾递归防止栈溢出。尾递归事实上和循环是等价的，
    没有循环语句的编程语言只能通过尾递归实现循环
** for
   当我们使用for循环时，只要作用于一个可迭代对象，for循环就可以正常运行，
   而我们不太关心该对象究竟是list还是其他数据类型
   判断一个对象是否是可迭代对象:
   from collections import Iterable
   isinstance('abc', Iterable) #=> True, 可迭代对象, False不可迭代对象

   enumerate可以将list变成索引-元素对
   for i, v in enumrate([1, 2, 3]):
       print i, v
** 生成器-yield
   创建生成器的方法
   1. 将一个列表生成式稍作变化即可,
      g = (s for s in xrange(11))
   2. 将一个函数变成生成器
      只需将return换成yield即可
      变成generator的函数，
       在每次调用next()的时候执行，遇到yield语句返回，再次执行时从上次返回的yield语句处继续执行
   可以利用isgeneratorfunction 来判断一个函数是否是一个特殊的迭代器
   from inspect import isgeneratorfunction
   isgeneratorfunction(fab) # true if fab is a generator function
   要注意区分fab与fab(5)， fab是一个generator function，而fab(5)调用返回的是一个generator
   例如：
   import types
   isinstance(fab, types.GeneratorType) # false
   isinstance(fab(5), types.GeneratorType) # true
   yield的另一个用处是读取大文件时防止占用大量内存
   def read_file(fpath):
       BLOCK_SIZE = 1024
       with open(fpath, 'rb') as f:
           while True:
               block = f.read(BLOCK_SIZE)
               if block:
                   yield block
               else:
                   return 
** 模块搜索路径
   import sys
   sys.path #=> 输出模块的加载路径
*** 添加修改路径
    1. 修改sys.path
       sys.path.append('/loadpath')
    2. 设置PYTHONPATH
** 使用__future__
   把下一个新版本的特性导入到当前版本，于是我们就可以在当前版本中测试一些新版本的特性
   例如:
   from __future__ import unicode_literals
   print isinstance('xxx', unicode) #=> True

   from __future__ import division
   10 / 3 #=> 3.3333
** 对象
   获取对象信息
   type(xx)
   python将每种type类型都定义好了常量, 放在types模块里, 使用之前, 导入即可
   import types
   if type('abc') == types.StringType: xx
   注意有一种类型就叫TypeType，所有类型本身的类型就是TypeType, 如:
   type(int) == type(str) == types.TypeType

   能用type()判断的基本类型也可以用isinstance()判断
   isinstance()还可以判断一个变量是否是某些类型中的一种
   isinstance('a', (str, unicode))
   利用hasattr(), getattr(), setattr()可以操作一个对象的状态
   如果在使用getattr()获取不存在的属性时, 会报错, 可以设定一个默认值
   getattr(obj, 'z', 404) #属性z不存在, 就返回404
   getattr() 也可以获取对象的方法
*** __slots__   
    想限制类的属性, 就需要用到__slots__变量
    __slots__定义的属性仅对当前类起作用，对继承的子类是不起作用的
    除非在子类中也定义__slots__,这样,子类允许定义的属性就是自身的__slots__加上父类的__slots__

    如果父类没有定义__slots__, 则子类定义的__slots__的不起作用
    一个使用__slots__减少内存占用的案例：http://blog.jobbole.com/52420/
    默认情况下，Python用一个dict来存储对象实例的属性。这在一般情况下还不错，而且非常灵活，
    乃至你在运行时可以随意设置新的属性。但是，对一些在”编译”前就知道该有几个固定属性的小class来说，
    这个dict就有点浪费内存了。
    还可以用collections.namedtuple，它允许访问参数，但只占用一个tuple的空间。
*** 对类的所有属性和方法调用全部动态化, __getattr__
    只有在没有找到属性的情况下, 才调用__getattr__, 已有的属性, 不会在__getattr__
    中查找, 利用该方法可以动态的调用
    class Chain(object):
        def __init__(self, path=''):
            self._path = path

        def __getattr__(self, path):
            return Chain('%s/%s' % (self._path, path))

        def __str__(self):
            return self._path
    具体使用参看codesegment/objecttypes.py
*** 可调用
    通过callable(), 可以判断出一个对象是否是"可调用"对象
    任何类，只需要定义一个__call__()方法，就可以直接对实例进行调用
    注意：在使用callable判断类的实例是否是可以调用的，类本身是
    可调用的。即只有定义了__call__的类的实例才是可调用的。
*** Mixin
    如果需要“混入”额外的功能，通过多重继承就可以实现，
    比如，让Ostrich除了继承自Bird外，再同时继承Runnable。这种设计通常称之为Mixin。
    为了更好地看出继承关系，我们把Runnable和Flyable改为RunnableMixin和FlyableMixin。
    类似的，你还可以定义出肉食动物CarnivorousMixin和植食动物HerbivoresMixin，
    让某个动物同时拥有好几个Mixin
    
    class Dog(Mammal, RunnableMixin, CarnivorousMixin):
        pass
*** 定制类
    通过实现类的特定方法, 如__str__, __len__等等方法
    __str__: 将类返回一个好看的格式
    __iter__: 类可以迭代, 此时还要定义next方法
    __getitem__: 类可以当做list来取数据, 此时入股传入的是一个slice, 需要对slice做特殊处理
    例如:
    class Fib(object):
    def __getitem__(self, n):
        if isinstance(n, int):
            a, b = 1, 1
            for x in range(n):
                a, b = b, a + b
            return a
        if isinstance(n, slice):
            start = n.start
            stop = n.stop
            a, b = 1, 1
            L = []
            for x in range(stop + 1):
                if x >= start:
                    L.append(a)
                a, b = b, a + b
            return L
    __call__: 可以直接对实例进行调用,__call__()还可以定义参数。对实例进行直接调用就好比对一个函数进行调用一样
*** 使用type创建类    
    type()函数既可以返回一个对象的类型，又可以创建出新的类型
    参考codesegment/objecttypes.py
*** metaclass
    控制类的创建行为，还可以使用metaclass
    即:先定义metaclass，就可以创建类，最后创建实例
    参见 codesegment/metaclass.py
    一个理解元类的网址: http://blog.jobbole.com/21351/
    Python中创建类的内部机制是：
    1. 当看见一个类的定义，python首先搜集所有属性到一个字典中
    2. 当类结束定义，python将决定类的元类，姑且称它为Meta
    3. python 执行Meta(name, base, dct), 其中：
       Meta是元类，所以这个调用是实例化
       name是新建类的类名，bases是新建类的基类元素，多个值时用tupple表示
       dct将属性名映射到对象，列出所有的类属性
*** 类属性与实例属性
    直接在类中定义的属性就是类属性
    通过self.xx = xx的属性是实例属性
    在编写程序的时候，千万不要把实例属性和类属性使用相同的名字
** 错误处理
   常见的错误类型和继承关系:https://docs.python.org/2/library/exceptions.html#exception-hierarchy
*** 调试
    可以使用断言,assert. 启动python的时候可以使用-O参数来关闭assert
    使用logging
**** 使用pdb
     python -m pdb filename.py
     pdb定位到下一步要执行的代码 -> s = '0'。输入命令l来查看代码
     n单步执行
     p xx: 打印xx的值, xx可以是python合法的语句
     一步一步调试很费劲, 可以在源代码中使用如下方法来快速定位:
     import pdb
     在需要调试的代码前: pdb.set_trace()
     之后运行程序(python filename.py), 程序会自动在此处暂停并进入pdb调试环境
** 数据结构 
   http://blog.jobbole.com/65218/
*** collections
    collections模块包含了内建类型之外的一些有用的工具，
    例如Counter、defaultdict、OrderedDict、deque以及nametuple。
    其中Counter、deque以及defaultdict是最常用的类
**** deque
    deque是一种由队列结构扩展而来的双端队列(double-ended queue)，队列元素能够在队列两端添加或删除。
    因此它还被称为头尾连接列表(head-tail linked list)，尽管叫这个名字的还有另一个特殊的数据结构实现

    Deque支持线程安全的，经过优化的append和pop操作，在队列两端的相关操作都能够达到近乎O(1)的时间复杂度。
**** defaultdict
    这个类型除了在处理不存在的键的操作之外与普通的字典完全相同。
    当查找一个不存在的键操作发生时，它的default_factory会被调用，提供一个默认的值，
    并且将这对键值存储下来。其他的参数同普通的字典方法dict()一致，
    一个defaultdict的实例同内建dict一样拥有同样地操作。
    对于比较大的字典类型，调用keys(), values(), items()会构成同样巨大的列表， 建议
    使用迭代器代替，以减少内存开销, 例如：
    d.iterkeys(), d.itervalues(), d.iteritems()
*** array
    array模块定义了一个很像list的新对象类型，
    不同之处在于它限定了这个类型只能装一种类型的元素。array元素的类型是在创建并使用的时候确定的。

    如果你的程序需要优化内存的使用，并且你确定你希望在list中存储的数据都是同样类型的，
    那么使用array模块很合适。虽然说能够节省空间，array上几乎没有什么基本操作能够比在list上更快。

    在使用array进行计算的时候，需要特别注意那些创建list的操作。例如，使用列表推导式(list comprehension)的时候，
    会将array整个转换为list，使得存储空间膨胀。一个可行的替代方案是使用生成器表达式创建新的array。看代码：
    import array
    
    a = array.array("i", [1,2,3,4,5])
    b = array.array(a.typecode, (2*x for x in a))
    因为使用array是为了节省空间，所以更倾向于使用in-place操作。一种更高效的方法是使用enumerate：
    import array
    a = array.array("i", [1,2,3,4,5])
    for i, x in enumerate(a):
    a[i] = 2*x
    对于较大的array，这种in-place修改能够比用生成器创建一个新的array至少提升15%的速度。

    那么什么时候使用array呢？是当你在考虑计算的因素之外，
    还需要得到一个像C语言里一样统一元素类型的数组时。
    import array
    from timeit import Timer
    
    def arraytest():
        a = array.array("i", [1, 2, 3, 4, 5])
        b = array.array(a.typecode, (2 * x for x in a))
 
    def enumeratetest():
        a = array.array("i", [1, 2, 3, 4, 5])

    for i, x in enumerate(a):
        a[i] = 2 * x
 
    if __name__=='__main__':
        m = Timer("arraytest()", "from __main__ import arraytest")
        n = Timer("enumeratetest()", "from __main__ import enumeratetest")
 
        print m.timeit() # 5.22479210582
        print n.timeit() # 4.34367196717
*** heapq
    heapq模块使用一个用堆实现的优先级队列。堆是一种简单的有序列表，并且置入了堆的相关规则

    import heapq
    heap = []
 
    for value in [20, 10, 30, 50, 40]:
        heapq.heappush(heap, value)
 
    while heap:
        print heapq.heappop(heap)

    heapq有两个函数nlargest()和smallest()，两个函数也能够通过一个键参数使用复杂的数据结构
    portfolio = [
    {'name': 'IBM', 'shares': 100, 'price': 91.1},
    {'name': 'AAPL', 'shares': 50, 'price': 543.22},
    {'name': 'FB', 'shares': 200, 'price': 21.09},
    {'name': 'HPQ', 'shares': 35, 'price': 31.75},
    {'name': 'YHOO', 'shares': 45, 'price': 16.35},
    {'name': 'ACME', 'shares': 75, 'price': 115.65}
    ]

    cheap = heapq.nsmallest(3, portfolio, key = lambda s: s['price'])
    expensive = heapq.nlargest(3, portfolio, key = lambda s: s['price'])
        
    import heapq
    class Item:
        def __init__(self, name):
            self.name = name
 
        def __repr__(self):
            return 'Item({!r})'.format(self.name)
 
    class PriorityQueue:
        def __init__(self):
            self._queue = []
            self._index = 0
     
        def push(self, item, priority):
            heapq.heappush(self._queue, (-priority, self._index, item))
            self._index += 1
     
        def pop(self):
            return heapq.heappop(self._queue)[-1]
     
    q = PriorityQueue()
    q.push(Item('foo'), 1)
    q.push(Item('bar'), 5)
    q.push(Item('spam'), 4)
    q.push(Item('grok'), 1)
     
    print q.pop() # Item('bar')
    print q.pop() # Item('spam')
    print q.pop() # Item('foo')
    print q.pop() # Item('grok')
*** bisect
    bisect模块能够提供保持list元素序列的支持。它使用了二分法完成大部分的工作。
    它在向一个list插入元素的同时维持list是有序的。在某些情况下，这比重复的对一个list进行排序更为高效，
    并且对于一个较大的list来说，对每步操作维持其有序也比对其排序要高效。
*** weakref
    weakref模块能够帮助我们创建Python引用，却不会阻止对象的销毁操作。
    这一节包含了weak reference的基本用法，并且引入一个代理类。
    
    strong reference是一个对对象的引用次数、生命周期以及销毁时机产生影响的指针。
    strong reference如你所见，就是当你将一个对象赋值给一个变量的时候产生的：
    
    a = [1,2,3]
    b = a
    在这种情况下，这个列表有两个strong reference，分别是a和b。
    在这两个引用都被释放之前，这个list不会被销毁

    Weak reference则是对对象的引用计数器不会产生影响。当一个对象存在weak reference时，
    并不会影响对象的撤销。这就说，如果一个对象仅剩下weak reference，那么它将会被销毁。 

    你可以使用weakref.ref函数来创建对象的weak reference。
    这个函数调用需要将一个strong reference作为第一个参数传给函数，并且返回一个weak reference。
    例如：
    import weakref

    a = Foo()
    b = weakref.ref(a)

    一个临时的strong reference可以从weak reference中创建，即下例中的b()：
    a == b() 
    如果通过weakref来访问，需要使用如下方式：b().show() <==> a.show()
    当我们删除strong reference的时候，对象将立即被销毁

    使用weakref.proxy，就能提供相对于weakref.ref更透明的可选操作。
    同样是使用一个strong reference作为第一个参数并且返回一个weak reference，
    proxy更像是一个strong reference，但当对象不存在时会抛出异常。
    a = Foo()
    b = weakref.proxy(a)
    b.show() <==> a.show()

    最好将weak reference用于开销较大的对象，或避免循环引用(虽然垃圾回收器经常干这种事情)

    提示：只有library模块中定义的class instances、functions、methods、sets、frozen sets、
    files、generators、type objects和certain object types
    (例如sockets、arrays和regular expression patterns)支持weakref。
    内建函数以及大部分内建类型如lists、dictionaries、strings和numbers则不支持。
*** copy
    通过shallow或deep copy语法提供复制对象的函数操作
    对于shallow copy而言，它创建一个新的混合对象，并且将原对象中其他对象的引用插入新对象
    对于deep copy而言，它创建一个新的对象，并且递归地复制源对象中的其他对象并插入新的对象中
    普通的赋值操作只是简单的将新变量指向源对象
*** pprint
    pprint模块能够提供比较优雅的数据结构打印方式，如果你需要打印一个结构较为复杂，
    层次较深的字典或是JSON对象时，使用Pprint能够提供较好的打印结果
** 序列化
   Python提供两个模块来实现序列化：cPickle和pickle。
   这两个模块功能是一样的，区别在于cPickle是C语言写的，速度快，pickle是纯Python写的，速度慢，
   跟cStringIO和StringIO一个道理，用的时候，先尝试导入cPickle，如果失败，再导入pickle，例如：
   try:
       import cPickle as pickle
   except ImportError:
       import pickle

   pickle.dumps()方法把任意对象序列化成一个str，然后，就可以把这个str写入文件。
   或者用另一个方法pickle.dump()直接把对象序列化后写入一个file-like Object，例如：
   pickle.dump(data, filehandle)

   对象从磁盘读到内存时，可以先把内容读到一个str，然后用pickle.loads()方法反序列化出对象，
   也可以直接用pickle.load()方法从一个file-like Object中直接反序列化出对象
   data = pickle.load(filehandle)

   Pickle的问题和所有其他编程语言特有的序列化问题一样，就是它只能用于Python，
   并且可能不同版本的Python彼此都不兼容，因此，只能用Pickle保存那些不重要的数据，
   不能成功地反序列化也没关系。
** json
   python中的None会转换为json的null
   dumps()方法返回一个str，内容就是标准的JSON。

   类似的，dump()方法可以直接把JSON写入一个file-like Object
   要把JSON反序列化为Python对象，用loads()或者对应的load()方法，前者把JSON的字符串反序列化，
   后者从file-like Object中读取字符串并反序列化

   反序列化得到的所有字符串对象默认都是unicode而不是str。由于JSON标准规定JSON编码是UTF-8，
   所以我们总是能正确地在Python的str或unicode与JSON的字符串之间转换
*** 将class实例化为json
    可选参数default就是把任意一个对象变成一个可序列为JSON的对象，
    我们只需要为类专门写一个转换函数，再把函数传进去即可，例如：
    def student2dict(std):
        return {
            'name':std.name,
            'age': std.age',
            'score': std.score
        }
    print json.dumps(s, default=student2dict)
    但是如果还有其他的类，使用上诉方法就不好了，可以使用如下方法：
    print json.dumps(s, default=lambda obj: obj.__dict__)
    通常class的实例都有一个__dict__属性，它就是一个dict，用来存储实例变量。
    也有少数例外，比如定义了__slots__的class

    同理loads也一样，传入的object_hook函数负责转换为某个实例
** 多进程
   Linux/Unix的fork()调用一次返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），
   然后，分别在父进程和子进程内返回。
   子进程永远返回0，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，
   父进程要记下每个子进程的ID，而子进程只需要调用getppid()就可以拿到父进程的ID

   import os
   os.fork() # 由于windows没有fork，因此不能使用该方法来实现多进程
   multiprocessing 就是用来解决跨平台的多进程问题的
   multiprocessing模块提供了一个Process类来代表一个进程对象，
   下面的例子演示了启动一个子进程并等待其结束：
*** 使用进程池
    from multiprocessing import Pool

    import os, time, random
      
    def long_task(name):
        print 'run task %s (%s) ...' %(name, os.getpid())
        start = time.time()
        time.sleep(random.random() * 3)
        end = time.time()
        print 'task %s runs %0.2f seconds.' %(name, (end-start))
     
    if __name__ == '__main__':
        print 'parent process %s.' % os.getpid()
        p = Pool() #p = Pool(5) #设定5个进程池，默认为CPU的核数
        for i in xrange(5):
            p.apply_async(long_task, args=(i,))
        print 'waiting for all subprocess done...'
        p.close()
        p.join()
        print 'all done'
    对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须先调用close()，
    调用close()之后就不能继续添加新的Process了
*** 进程间通信
    Python的multiprocessing模块包装了底层的机制，提供了Queue、Pipes等多种方式来交换数据
    from multiprocessing import Queue, Process
    import os, random, time
     
    def write(q):
        for v in ['a', 'b', 'c', 'd']:
            q.put(v)
            time.sleep(random.random())
     
    def read(q):
        while True:
            value = q.get(True)
            print 'get %s from q' % value
     
    if __name__ == '__main__':
        q = Queue()
        p1 = Process(target=write, args=(q,))
        p2 = Process(target=read, args=(q,))
     
        p1.start()
        p2.start()
        p1.join()
        p2.terminate() #read 是死循环，因此无法等待其结束，只能强行终止
    由于Windows没有fork调用，因此，multiprocessing需要“模拟”出fork的效果，
    父进程所有Python对象都必须通过pickle序列化再传到子进程去，所有，
    如果multiprocessing在Windows下调用失败了，要先考虑是不是pickle失败了
** 多线程    
   Python的线程是真正的Posix Thread，而不是模拟出来的线程
   thread和threading，thread是低级模块，threading是高级模块，对thread进行了封装。
   绝大多数情况下，我们只需要使用threading这个高级模块

   import time, threading

   def loop():
       print 'threading %s is running... ' % threading.current_thread().name
       n = 0
       while n < 5:
           print 'current thread %s >>>> %d' %(threading.current_thread().name, n)
           n += 1
       print 'current thread %s end' % threading.current_thread().name
    
   print 'thread %s is runnging...' % threading.current_thread().name
   t = threading.Thread(target=loop, name="loopthread")
   t.start()
   t.join()
   print 'thread %s end' % threading.current_thread().name

   Python的threading模块有个current_thread()函数，它永远返回当前线程的实例
   主线程实例的名字叫MainThread，子线程的名字在创建时指定
   字仅仅在打印时用来显示，完全没有其他意义，如果不起名字Python就自动给线程命名为Thread-1，Thread-2
   多线程中，所有变量都由所有线程共享，所以，任何一个变量都可以被任何一个线程修改，因此，
   线程之间共享数据最大的危险在于多个线程同时改一个变量，把内容给改乱了
   解决方法是加锁
   balance = 0
   lock = threading.Lock()
    
   def run_thread(n):
       for i in range(100000):
           # 先要获取锁:
           lock.acquire()
           try:
               # 放心地改吧:
               change_it(n)
           finally:
               # 改完了一定要释放锁:
               lock.release()
   当多个线程同时执行lock.acquire()时，只有一个线程能成功地获取锁，然后继续执行代码，其他线程就继续等待直到获得锁为止。
   Python的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁：Global Interpreter Lock，
   任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，
   让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，
   多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。

   在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，
   那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。

   Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。
   多个Python进程有各自独立的GIL锁，互不影响
** import 的顺序
   参考：http://legacy.python.org/dev/peps/pep-0008/#imports
   此处的import顺序只是建议，不是强制要求
   1. 标准库的import
   2. 相关的第三方库的import
   3. 当前项目中的import
      
* python 技巧
** 一行代码定义一颗树
   def tree(): return defaultdict(tree)
** 上下文管理器
   with关键词
*** 自定义上下文管理器
    要实现上下文管理器，必须实现两个方法--一个负责进入语句块的准备操作，一个是负责离开语句块的善后操作
    当一个对象被用作上下文管理器时：
    __enter__:方法将在进入代码块前被调用
    __exit__:方法在离开代码块后被调用，即时在代码中遇到了异常
    class PypixContextManagerDemo:
        def __enter__(self):
            print 'Entering the block'
         
        def __exit__(self, *unused):
            print 'Exiting the block'
 
    with PypixContextManagerDemo():
        print 'In the block'

    传递参数，可以使用类的__init__方法
    class PypixOpen:
        def __init__(self, filename, mode):
            self.filename = filename
            self.mode = mode
        def __enter__(self):
            pass
        def __exit__(self, *unused):
            pass

        with PypixOpen(filename, mode) as writer:
            writer.write("Hello world from our new context manager")
    处理异常
    如果语句块内部发生了异常，__exit__方法将会被调用，而异常将会被重新抛出
    完全的__exit__函数签名是：
    def __exit__(self, exc_type, exc_val, ext_tb)
    这样__exit__函数就能够拿到关于异常的所有信息(异常类型，异常值以及异常追踪信息)，
    这些信息将帮助异常处理操作。
*** contextlib的内容
    contexlib是一个python模块，作用是提供更易用的上下文管理器
    例如处理数据库：
    with contextlib.closing(createdatabase()) as database:
        database.query()
    contextlib.closing方法将在语句块作用结束后调用数据库的关闭方法 

    contextlib.nested可以帮助减少嵌套的调用，例如：
    with open('rfile', 'r') as reader:
        with open('wfile', 'w') as writer:
            writer.writer(reader.read())
    上述方法是不提倡使用的，此时可以用如下方法代替：
    with contextlib.nested(open('fread', 'r'), open('fwrite', 'w')) as (reader, writer):
        writer.writer(reader.read())
    在python2.7中上述方法可以简写为：
    with open('fread', 'r') as reader, open('fwrite‘, 'w') as writer:
        writer.writer(reader.read())
    对于python高级玩家来说，任何能够被yield关键词分割成两部分的函数，都能够通过装饰器的上下文管理器来实现，
    任何在yield之前的内容都可以看做在代码块之前执行，yield后的代码可以看做放在exit函数中
    例如线程锁的例子：
    import threading
    lock = threading.Lock()
    def safewrite(file, content):
        lock.acquire()
        file.write(content)
        lock.release()
    使用上下文管理来实现：
    @contextlib.contextmanager
    def loudLock():
        print "locking"
        lock.acquire()
        yield
        print 'releasing'
        lock.release()

    with loudLock():
        print 'lock is locked: %s' % lock.locked()
        print 'doing something that needs locking'
        
    注意：这不是异常安全的写法，如果想保证异常安全，可以对yield使用try语句，threading.lock是一个上下文管理
    器了，只需简单的处理就行，如下：
    @contextlib.contextmanager
    def loudLock():
        print 'locking'
        with lock:
            yield
        print 'releasing'
    此时当threading.lock在异常发生时会通过__exit__函数返回false
    如果你希望在上下文管理器中使用“as”关键字，那么就用yield返回你需要的值，它将通过as关键字赋值给新的变量。
** 列表推导
   def sgen(optional_parameter):
       return (x ** 2 for x in num if x > optional_parameter)
   sgen(0) #是一个generator
** 装饰器
*** 类装饰器
    对装饰器的类实现唯一要求是它必须能如函数一般使用，也就是说它必须是可调用的。
    所以，如果想这么做这个类必须实现__call__方法。
** 描述器
   描述器很好的总结了Python中的绑定方法(bound method)这个概念，绑定方法是经典类(classic classes)的
   实现核心。在经典类中，当在一个对象实例的字典中没有找到某个属性时，会继续到类的字典中查找，
   然后再到基类的字典中，就这么一直递归的查找下去。如果在类字典中找到这个属性，
   解释器会检查找到的对象是不是一个Python函数对象。如果是，则返回的并不是这个对象本身，
   而是返回一个柯里化(currying function)的包装器对象。当调用这个包装器时，它会首先在参数列表之前插入实例，
   然后再调用原函数。
   柯里化：http://zh.wikipedia.org/wiki/%E6%9F%AF%E9%87%8C%E5%8C%96
   function，method，bound method及unbound method的区别。首先，函数(function)是由def或lambda创建的。
   当一个函数在class语句块中定义或是由type来创建时，它会转成一个非绑定方法(unbound method)，
   而当通过类实例(instance)来访问此方法的时候，它将转成绑定方法(bound method)，
   绑定方法会自动将实例作为第一个参数传入方法。综上所述，方法是出现在类中的函数，
   绑定方法是一个绑定了具体实例的方法，反之则是非绑定方法。
** 处理编码问题
   由于在python3中字符串默认使用unicode，如果想在python 2.7的版本中也默认使用unicode的话，可以
   使用如下代码：
   from __future__ import unicode_literals

   Python还提供了一个codecs模块帮我们在读文件时自动转换编码，直接读出unicode：
   import codecs
   with codecs.open('/Users/michael/gbk.txt', 'r', 'gbk') as f:
       f.read() # u'\u6d4b\u8bd5'

   一个引入unicode_literals的编码错误
   详见：http://www.the5fire.com/unicodeencodeerror-from-future.html

   #encoding:utf8
   #from __future__ import unicode_literals #一旦引入该模块，则会出现编码错误
   from datetime import datetime

   now = datetime.now()
   print now.strftime('%m月%d日 %H:%M')

   解决方法一：设置运行时编码为utf8
   #encoding:utf8

   from __future__ import unicode_literals #一旦引入该模块，则会出现编码错误
   from datetime import datetime

   import sys
   reload(sys)
   system.setdefaultencoding('utf-8')

   now = datetime.now()
   print now.strftime('%m月%d日 %H:%M')
   解决方案二：使用byte string

   #encoding:utf8
   from __future__ import unicode_literals #一旦引入该模块，则会出现编码错误
   from datetime import datetime

   now = datetime.now()
   print now.strftime(b'%m月%d日 %H:%M')

   #或者这样也行
   t = bytearray('%m月 %d %H:%M', 'utf-8')
   print now.strftime(str(t))
** A collection of not-so-obvious Python stuff you should know   
   http://nbviewer.ipython.org/github/rasbt/python_reference/blob/master/not_so_obvious_python_stuff.ipynb
*** The C3 class resolution algorithm for multiple class inheritance
    python中，类继承自object的类是新式类，类似class D: pass的类是旧式类。
    新式类的mro算法采用的是C3 算法，
    应用在Python中是为了解决原来基于深度优先搜索算法不满足本地优先级，和单调性的问题
    因此在定义类时，最好使用新式类的方式
    <code>
*** using += on lists
    Python lists are mutable objects as we all know. So, if we are using the += operator on lists,
    we extend the list by directly modifying the object directly
    However, if we use the assigment via my_list = my_list + ..., we create a new list object
    the .append() and .extends() methods are modifying the list object in place
*** true and false in the datetime module
    bool(datetime.time(0,0,0)) --> false
    bool(datetime.time(1,0,0)) --> true
** python 导入其他模块
   当使用import 导入其他模块时, __name__的值是该模块的名字，
   如果直接运行某个模块，则__name__的值为__main__
* 测试
  在python中一个函数就可以认为是一个单元
  测试文件的例子：
  import unittest
  #import 想要测试的模块

  class xx(unittest.TestCase):
      def test_xx(参数):
          self.assertTrue(函数名(参数))

  if __name__ == '__main__':
      unittest.main()
  如果想执行测试函数，则该函数应该以test_开头命名
* 函数对象
  def foo(a):
      x = 3
      return x + a

  foo.func_code
  dir(foo.func_code)
  foo.func_code.co_varnames #获取函数的变量名
  foo.func_code.co_consts #函数中已知的常量
  foo.func_code.co_argcount #函数参数的数量
  foo.func_code.co_code #获取函数字节码
  读取字节码的值
  [ord(b) for b in foo.func_code.co_code]
  字节码本身并不包含任何python对象，或引用任何对象
  使用dis函数分析foo函数
  import dis
  dis.dis(foo.func_code) # <--> dis.dis(foo)
  输出的数据格式说明：
  左边那一列数字是原始源代码的行号。第二列是字节码的偏移量：LOAD_CONST在第0行，STORE_FAST在第3行，
  以此类推。中间那列是字节的名字。它们是为程序员所准备的——解释器是完全不需要的。
  最后两列告诉我们一些关于指令参数（如果有的话）的细节。第四列是参数本身。
  它表示一个指向代码对象其它属性的索引。在这个例子中，LOAD_CONST的参数指向列表co_consts，
  STORE_FAST的参数指向co_varnames。dis在第四列所指向的的地方查找常数或者名称,
  最后在第五列返回给我们它找到的数据。这很容易就能得到证实了：
  foo.func_code.co_consts[1] #3
  foo.func_code.co_varnames[1] #'x'
** 一个有意思的问题
   def modulus(x, y):
       return x % y

   modulus('hello %s', 'world') # --> hello world，能顺利执行输出，为什么不报错？答案见下：
   反编译modulus函数，dis.dis(modulus)，会看到反编译后的代码中有一个语句BINARY_MODULO，
   当BINARY_MODULO处理两个字符串的时候，它默认执行字符串插值而不是求余数，因此会出现上述情况
* Advanced regular expression tips and techniques
  http://blog.jobbole.com/65605/
  python 可以通过对re.compile 或 re.match 设置 re.DEBUG (实际上就是整数 128) 标志
  就可以输出正则表达式的解析树
** greedy
   when greedy, the quantifiers (such as * or +) match as many character as possible
   add a question mark(?) after the quantifier (.*?) it becomes “ungreedy”
** lookahead and lookbehind assertions
   <code --lookahead>
** coditional(if-then-else) patterns
   Regular expressions provide the functionality for checking certain conditions.
   The format is as follows:
   (?(?=regex)then|else)
   The condition can be a number. In which case it refers to a previously captured subpattern
   <code -- conditional>
** no caputre subpatterns
   <code -- no caputre>
** named subpatterns
   <code -- named subpatterns>
** using callbacks
   In Python re.sub() can be used to add callback functionality to regular expression replacements
   <code --callbacks>
   
* 第三方库
** docopt
   docopt是一个第三方开发的模块，用于创建command line interface
* 命令行参数解析
  <commandline.py>
** python调用系统命令或者脚本
   <invokecmd.py>
* pymongo的使用
** 将mongo查询的数据转换为  
* Python 环境搭建
** 虚拟环境搭建
   http://www.the5fire.com/virtualenv-python-env.html
*** 工具
    sudo aptitude install python-dev
    该工具是virtualenv，使用python开发的一个创建虚拟环境的工具
    ubuntu安装的命令如下：
    apt-get install python-virtualenv/ sudo pip install virtualenv
*** 使用
    先建立一个目录： mkdir testvirtual => cd testvirtual
    创建虚拟环境：virtualenv env1 => cd env1 => source bin/activate
    此时会发现shell提示符前多了个env1提示，表明已经是在虚拟环境中了，在该环境中
    可以任意安装python库，而不用担心会把系统自带的python库搞乱
*** 另外一个工具
    virtualenvwrapper，可以通过sudo pip install virtualenvwrapper来安装。
    安装完之后，需要在用户根目录下的.bashrc末尾加入：
    source /usr/local/bin/virtualenvwrapper.sh
    设置好之后，可以使用如下方法操作虚拟环境：
    mkvirtualenv env1
    退出环境：deactivate
    进入已经存在的环境或者切换环境： workon env1 或 env2
    删除环境： rmvirtualenv env1
** 软件安装
*** lnmp 的安装
    www.lnmp.org下载最新的源码，阅读readme查看安装方法
*** setuptools
    该软件主要是需要在安装mysqldb时需要该软件，下载setuptools-0.6c11.tar.gz即可
*** mysqldb
    下载mysqldb，在安装mysqldb前，需要先安装libmysqlclient-dev
    sudo aptitude install libmysqlclient-dev，
    修改mysqldb中的site.cfg文件，添加mysql_config文件的路径，该文件的路径可以
    使用命令which mysql_config来确定
*** redis
    在官网下载最新的redis源码，执行make即可安装，
    redis的启动:redis-server redis.conf
    py-redis: https://github.com/andymccurdy/redis-py
* Python的名字空间
  名字空间时python最核心的内容
  python的名字实际上是一个字符串对象，它和所指的目标对象一起在名字空间中构成一项
  {name:object}关联，python有很多名字空间， 例如：globals的模块名字空间，
  locals的函数堆栈名字空间，还有class，instance名字空间，不同的名字空间决定对象
  的作用域和生命周期。
  在函数外部，locals()和globals()作用完全相同， 在函数内部调用时，locals()则是
  获取当前函数堆栈的名字空间,其存储的时函数参数，局部变量等信息。
  可以通过<module>.__dict__访问其他模块的名字空间。
  
